### 250318 TIL

## 1. **CutMix와 일반 데이터 증강을 동시에 활용하는 방법**
- 일반 `ImageDataGenerator`와 `CutMixGenerator`를 **동시에 사용하고 싶을 때** 주의할 점
- `train_generator + cutmix_train_generator`는 동작하지 않음 → 별도의 **커스텀 제너레이터** 필요
- 해결 방법:
  1. **단계별 학습** (초반에는 일반, 후반에는 CutMix 사용)
  2. **커스텀 제너레이터**를 만들어 한 에포크 안에서 두 가지 데이터를 섞어 사용

## 2. **steps_per_epoch 개념**
- 한 에포크 동안 **몇 번(batch)을 업데이트할지** 결정하는 값
- 기본적으로 `steps_per_epoch = (전체 데이터 개수 / batch_size)`
- 너무 작게 설정하면 데이터 일부만 학습될 수 있음, 너무 크게 설정하면 중복 학습될 가능성 있음

## 3. **Early Stopping 평가 & 적절한 종료 시점**
- `EarlyStopping(monitor="val_accuracy", patience=3, mode="max")` 설정 시,
  - **3번 연속 val_accuracy 개선이 없으면 학습 중단**
- 현재 학습 로그에서 **val_accuracy가 90~91%에서 정체** → 학습 종료가 적절해 보임
- **val_loss도 감소하지 않고 유지** → 오버피팅 가능성

## 4. **더 학습할 가치가 있는지 판단하는 기준**
✅ Early Stopping이 발동했을 때 다음을 확인:
- **val_accuracy가 더 이상 오르지 않는가?** → 네
- **val_loss가 다시 증가하는가? (과적합 가능성)** → 네
- **학습률(lr)이 줄어들면서 개선 가능성이 낮은가?** → 네

➡ **현재 상태에서 더 학습해도 성능 향상이 어려울 가능성이 큼**

## 5. **추가 실험 아이디어**
- Fine-tuning 진행 (일부 레이어를 다시 학습하도록 설정, `lr` 줄여서 추가 학습)
  ```python
  for layer in model.layers[30:]:  
      layer.trainable = True
  ```
- 데이터 증강(Augmentation) 강화를 고려 (CutMix 비율 증가, 더 강한 변형 적용)
- 모델 구조 변경 고려 (ResNet50 → ResNet101, EfficientNet 등 업그레이드)

---
**결론:**
현재 학습이 **적절한 시점에서 멈춘 것으로 보이며**, 추가 학습보다는 **Fine-tuning 또는 데이터 증강 개선**이 더 효과적일 가능성이 높음.

