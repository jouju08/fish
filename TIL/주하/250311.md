# ResNet 개요

## 1. ResNet (Residual Network) 개념
### 개념
ResNet(Residual Network)은 **딥러닝 모델에서 매우 깊은 신경망을 안정적으로 학습할 수 있도록 설계된 아키텍처**이다.  
기본적인 CNN(합성곱 신경망) 모델이 깊어질수록 **기울기 소실(Vanishing Gradient)** 문제를 겪는 것을 해결하기 위해 **Residual Block(잔차 블록)** 을 도입하였다.

ResNet은 2015년 **마이크로소프트(Microsoft)** 연구팀이 발표한 모델로, ILSVRC(ImageNet Large Scale Visual Recognition Challenge)에서 우승을 차지하면서 널리 사용되었다.

---

## 2. ResNet의 특징
### Residual Block (잔차 블록)
- **잔차(Residual)** 를 학습하도록 설계된 구조로, **입력 값이 출력으로 그대로 전달될 수 있는 경로(Skip Connection)** 를 제공한다.
- 일반적인 CNN과 달리, `F(x) + x` 형태로 연산하여 **기울기 소실 문제를 해결하고 깊은 네트워크에서도 학습이 가능**하다.

### 다양한 ResNet 모델
| 모델명  | 레이어 수 | 주요 특징 |
|---------|----------|-----------|
| **ResNet-18**  | 18개 | 비교적 얕은 구조, 연산량이 적어 경량 모델에 적합 |
| **ResNet-34**  | 34개 | ResNet-18보다 깊지만, 여전히 연산량이 낮음 |
| **ResNet-50**  | 50개 | **Bottleneck 구조 사용**으로 연산량 최적화 |
| **ResNet-101** | 101개 | 더 깊은 구조로, 복잡한 특징을 학습 가능 |
| **ResNet-152** | 152개 | 가장 깊은 구조, 성능은 높지만 연산량이 큼 |

### Bottleneck 구조
- **ResNet-50 이상 모델**에서는 **3개의 Convolution 연산을 한 블록으로 묶는 Bottleneck 구조**를 사용한다.
- `1x1 → 3x3 → 1x1` 형태의 합성곱을 통해 **연산량을 줄이면서도 깊은 네트워크를 유지**할 수 있다.

### Identity Shortcut (Skip Connection)
- 일반적인 CNN 네트워크와 다르게 **이전 층의 출력을 그대로 다음 층으로 전달하는 Skip Connection이 포함**된다.
- 이 구조 덕분에 **기울기 소실 문제를 해결**하고 **더 깊은 네트워크 학습이 가능**하다.

---

## 3. ResNet의 장점
### 깊은 네트워크에서도 학습 가능
- Residual Block을 통해 **딥 네트워크(100층 이상)에서도 효과적으로 학습 가능**하다.
- 일반적인 CNN 모델보다 **훨씬 깊은 모델을 만들 수 있어 성능이 뛰어남**.

### 기울기 소실(Vanishing Gradient) 문제 해결
- 기존 딥러닝 모델에서는 네트워크가 깊어질수록 **역전파 시 기울기 값이 0에 가까워지는 문제**가 발생했다.
- ResNet의 **Skip Connection은 이 문제를 해결하여 더 깊은 네트워크에서도 안정적인 학습을 보장**한다.

### 다양한 컴퓨터 비전(Task)에 활용 가능
- **이미지 분류(Classification), 객체 검출(Object Detection), 이미지 분할(Segmentation)** 등 다양한 작업에서 성능이 우수하다.
- **ResNet-50**은 전이 학습(Transfer Learning)에서 가장 널리 사용되는 모델 중 하나다.

---

## 4. ResNet의 단점
### 연산량(Computational Cost)이 많음
- **ResNet-50 이상의 모델은 연산량이 많아 학습 속도가 느릴 수 있다.**
- 특히 ResNet-101, ResNet-152는 고성능 GPU가 없으면 학습이 어렵다.

### 파라미터 개수가 많아 메모리 사용량이 큼
- 깊이가 깊어질수록 학습해야 할 파라미터 개수가 급격히 증가한다.
- 모델 크기가 커지면서, 작은 디바이스(예: 스마트폰)에서는 사용하기 어려울 수 있다.

### 네트워크가 너무 깊어지면 성능 향상이 제한적
- ResNet-152 이상의 모델은 **네트워크를 깊게 만들었을 때 성능 향상이 제한적인 경우가 있음**.
- 너무 깊은 네트워크는 오히려 **과적합(Overfitting) 가능성이 증가**할 수 있다.

---

## 5. ResNet의 활용 사례 (언제 사용하는가?)
### ResNet이 효과적인 경우
- **이미지 분류(Classification)** → ImageNet 같은 대형 데이터셋에서 높은 성능을 보임.
- **객체 검출(Object Detection)** → Faster R-CNN, Mask R-CNN 등의 백본(Backbone) 모델로 사용됨.
- **이미지 세분화(Segmentation)** → DeepLab, U-Net 등에서 백본으로 활용 가능.
- **전이 학습(Transfer Learning)** → 사전 학습된 가중치를 활용해 작은 데이터셋에서도 좋은 성능을 얻을 수 있음.

### ResNet 대신 다른 모델을 고려해야 하는 경우
- **경량 모델이 필요한 경우** → MobileNet, EfficientNet 등의 경량 네트워크가 더 적합함.
- **실시간 처리가 필요한 경우** → ResNet은 연산량이 많아 실시간 애플리케이션에는 부적합할 수 있음.

---

## 6. ResNet과 정확도의 관계
| 모델    | Top-1 Accuracy (ImageNet) | Top-5 Accuracy (ImageNet) |
|---------|-------------------------|-------------------------|
| ResNet-18  | 69.8%  | 89.1%  |
| ResNet-34  | 73.3%  | 91.4%  |
| ResNet-50  | 76.0%  | 92.9%  |
| ResNet-101 | 77.4%  | 93.5%  |
| ResNet-152 | 78.3%  | 94.1%  |

✔ **모델이 깊어질수록 정확도가 증가하지만, 연산량도 크게 증가한다.**
✔ **일반적으로 ResNet-50이 정확도와 연산량의 균형이 가장 좋은 모델로 평가된다.**
✔ **ResNet-101, ResNet-152는 더 높은 성능을 원할 때 사용되지만, 속도가 느려질 수 있다.**

---


- ResNet은 **Residual Block을 활용하여 깊은 네트워크에서도 효과적으로 학습할 수 있는 강력한 모델**이다.
- 깊은 네트워크에서도 **기울기 소실 문제를 해결하여 안정적으로 학습 가능**하다.
- 하지만 **연산량이 많고, 너무 깊어질 경우 성능 향상이 제한적일 수 있다.**
- **전이 학습(Transfer Learning)에서 매우 효과적이며, 다양한 컴퓨터 비전 문제에서 널리 사용된다.**

